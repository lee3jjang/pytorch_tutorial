{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.5 64-bit"
  },
  "interpreter": {
   "hash": "11f1dc213e07634baa4c5c321dec03c05dafae643c50f20e6d1a492290c05dc2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([0., 0., 0.])\ntensor([ 1.3755, -0.6023, -4.8127], grad_fn=<AddBackward0>)\ntensor(0.6819, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor(0.6819, grad_fn=<NegBackward>)"
      ]
     },
     "metadata": {},
     "execution_count": 134
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "x = torch.ones(5)\n",
    "y = torch.Tensor([0, 0, 0])\n",
    "W = torch.randn(5, 3, requires_grad=True)\n",
    "b = torch.randn(3, requires_grad=True)\n",
    "z = x@W+b\n",
    "print(y)\n",
    "print(z)\n",
    "loss = F.binary_cross_entropy_with_logits(z, y)\n",
    "print(loss)\n",
    "yp = 1/(1+torch.exp(-z))\n",
    "-(y*torch.log(yp)+(1-y)*torch.log(1-yp)).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[ 4.3970e-01,  1.1241e-01,  6.4079e-01,  4.4116e-01],\n        [-2.1586e-01, -7.4255e-01,  5.6272e-01,  2.5963e-01],\n        [ 5.2286e-01,  2.3022e+00, -1.4689e+00, -1.5867e+00],\n        [ 1.2032e+00,  8.4535e-02, -1.2001e+00, -4.7857e-03],\n        [-2.3034e-01, -3.9175e-01,  5.4329e-01, -3.9516e-01],\n        [ 2.0553e-01, -4.5033e-01, -5.7308e-01, -5.5536e-01],\n        [-1.1256e+00, -3.1700e-01, -1.0925e+00, -8.5194e-02],\n        [-9.3348e-02,  6.8705e-01, -8.3832e-01,  8.9182e-04],\n        [-7.5043e-01,  1.8541e-01,  6.2114e-01,  6.3818e-01],\n        [-2.4600e-01,  2.3025e+00, -1.8817e+00, -4.9727e-02]])\ntensor([2, 2, 3, 0, 2, 3, 1, 0, 1, 2])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(10, 4)\n",
    "target = torch.randint(4, size=(10,), dtype=torch.long)\n",
    "print(x)\n",
    "print(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[0., 0., 1., 0.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 0., 1., 0.]])"
      ]
     },
     "metadata": {},
     "execution_count": 119
    }
   ],
   "source": [
    "y = torch.zeros(10, 4)\n",
    "y[range(10), target] = 1\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# binary cross entropy with logit\n",
    "# class가 여러개면 -> target을 one hot encoding 해\n",
    "# 예측한 z도 같은 판떼기에서 나오잖아\n",
    "# z를 sigmoid 넣어줘 (x)\n",
    "# y*log(x)+ (1-y)*log(1-x)\n",
    "# 위에꺼를 -mean을 쳐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor(0.7800)"
      ]
     },
     "metadata": {},
     "execution_count": 127
    }
   ],
   "source": [
    "y_pred = 1/(1+torch.exp(-x))\n",
    "loss = -(y*torch.log(y_pred)+(1-y)*torch.log(1-y_pred)).mean()\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor(0.7800)"
      ]
     },
     "metadata": {},
     "execution_count": 130
    }
   ],
   "source": [
    "F.binary_cross_entropy_with_logits(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[ 1.5410, -0.2934, -2.1788],\n        [ 0.5684, -1.0845, -1.3986],\n        [ 0.4033,  0.8380, -0.7193],\n        [-0.4033, -0.5966,  0.1820],\n        [-0.8567,  1.1006, -1.0712]], requires_grad=True)\ntensor([ 0.1227, -0.5663,  0.3731], requires_grad=True)\ntensor([ 1.3755, -0.6023, -4.8127], grad_fn=<AddBackward0>)\ntensor(0.6819, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "torch.manual_seed(0)\n",
    "x = torch.ones(5)\n",
    "y = torch.Tensor([0, 0, 0])\n",
    "W = torch.randn(5, 3, requires_grad=True)\n",
    "b = torch.randn(3, requires_grad=True)\n",
    "z = x@W+b\n",
    "loss = F.binary_cross_entropy_with_logits(z, y)\n",
    "optimizer = torch.optim.SGD([W, b], lr=0.9)\n",
    "print(W)\n",
    "print(b)\n",
    "print(z)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(tensor([[ 1.3015, -0.3996, -2.1812],\n",
       "         [ 0.3290, -1.1907, -1.4010],\n",
       "         [ 0.1639,  0.7319, -0.7217],\n",
       "         [-0.6428, -0.7028,  0.1796],\n",
       "         [-1.0962,  0.9945, -1.0736]], grad_fn=<SubBackward0>),\n",
       " tensor([-0.1168, -0.6725,  0.3707], grad_fn=<SubBackward0>))"
      ]
     },
     "metadata": {},
     "execution_count": 178
    }
   ],
   "source": [
    "optimizer.zero_grad()\n",
    "loss.backward()\n",
    "W-W.grad*0.9, b-b.grad*0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[ 1.3015, -0.3996, -2.1812],\n        [ 0.3290, -1.1907, -1.4010],\n        [ 0.1639,  0.7319, -0.7217],\n        [-0.6428, -0.7028,  0.1796],\n        [-1.0962,  0.9945, -1.0736]], requires_grad=True)\ntensor([-0.1168, -0.6725,  0.3707], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(W)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(tensor([[0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.]]),\n",
       " tensor([0., 0., 0.]))"
      ]
     },
     "metadata": {},
     "execution_count": 185
    }
   ],
   "source": [
    "optimizer.zero_grad()\n",
    "W.grad, b.grad"
   ]
  }
 ]
}